{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6ee2c4a",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data\n",
    "\n",
    "First, let's import what we need and prepare the data (building on Exercise 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd1b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77b81a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data (same as Exercise 2)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='../data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41abe13",
   "metadata": {},
   "source": [
    "## 2. Understanding the Network Architecture\n",
    "\n",
    "We'll build a simple **feedforward neural network** with:\n",
    "- **Input layer**: 28×28 = 784 pixels (flattened)\n",
    "- **Hidden layer 1**: 128 neurons with ReLU activation\n",
    "- **Hidden layer 2**: 64 neurons with ReLU activation\n",
    "- **Output layer**: 10 neurons (one for each digit 0-9)\n",
    "\n",
    "```\n",
    "Input (784) → Dense(128) → ReLU → Dense(64) → ReLU → Dense(10) → Output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0095755c",
   "metadata": {},
   "source": [
    "## 3. Define the Neural Network Class\n",
    "\n",
    "In PyTorch, we create neural networks by inheriting from `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde97b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)  # Input layer to hidden layer 1\n",
    "        self.fc2 = nn.Linear(128, 64)        # Hidden layer 1 to hidden layer 2\n",
    "        self.fc3 = nn.Linear(64, 10)         # Hidden layer 2 to output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Flatten the input (batch_size, 1, 28, 28) -> (batch_size, 784)\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        \n",
    "        # Pass through the network\n",
    "        x = F.relu(self.fc1(x))  # First hidden layer with ReLU activation\n",
    "        x = F.relu(self.fc2(x))  # Second hidden layer with ReLU activation\n",
    "        x = self.fc3(x)          # Output layer (no activation here - will use softmax in loss)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"Neural network class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806a4224",
   "metadata": {},
   "source": [
    "## 4. Create an Instance of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b653f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = SimpleNN()\n",
    "\n",
    "print(\"Model created!\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba4212",
   "metadata": {},
   "source": [
    "## 5. Examine the Model Architecture\n",
    "\n",
    "Let's understand what's inside our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1579cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed layer information\n",
    "print(\"Model Architecture:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, layer in model.named_children():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Type: {type(layer).__name__}\")\n",
    "    if hasattr(layer, 'in_features'):\n",
    "        print(f\"  Input features: {layer.in_features}\")\n",
    "        print(f\"  Output features: {layer.out_features}\")\n",
    "        print(f\"  Parameters: {layer.in_features * layer.out_features + layer.out_features:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdf6f9a",
   "metadata": {},
   "source": [
    "## 6. Count Model Parameters\n",
    "\n",
    "Let's see how many trainable parameters our model has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74041b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "total_params = count_parameters(model)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")\n",
    "\n",
    "# Break down by layer\n",
    "print(\"\\nParameters by layer:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name:10s}: {param.numel():7,} parameters, shape: {list(param.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ee9ac4",
   "metadata": {},
   "source": [
    "## 7. Test the Forward Pass\n",
    "\n",
    "Let's verify that our model can process a batch of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468ba9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of images\n",
    "batch_images, batch_labels = next(iter(train_loader))\n",
    "\n",
    "print(f\"Input shape: {batch_images.shape}\")\n",
    "\n",
    "# Pass through the model\n",
    "with torch.no_grad():  # No gradient computation needed for testing\n",
    "    output = model(batch_images)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nOutput breakdown:\")\n",
    "print(f\"  - Batch size: {output.shape[0]}\")\n",
    "print(f\"  - Number of classes: {output.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda4bf5a",
   "metadata": {},
   "source": [
    "## 8. Examine Model Predictions (Untrained)\n",
    "\n",
    "Let's see what the untrained model predicts. It should be random!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e779d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at outputs for the first image\n",
    "print(\"Raw output scores for first image:\")\n",
    "print(output[0])\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = F.softmax(output[0], dim=0)\n",
    "print(\"\\nProbabilities (should be roughly uniform for untrained model):\")\n",
    "for i, prob in enumerate(probabilities):\n",
    "    print(f\"  Digit {i}: {prob.item():.3f} ({prob.item()*100:.1f}%)\")\n",
    "\n",
    "# Get prediction\n",
    "predicted_digit = output[0].argmax().item()\n",
    "actual_digit = batch_labels[0].item()\n",
    "\n",
    "print(f\"\\nPredicted digit: {predicted_digit}\")\n",
    "print(f\"Actual digit: {actual_digit}\")\n",
    "print(f\"Correct: {predicted_digit == actual_digit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b231ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions from the untrained model\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle('Untrained Model Predictions (Expect Random Guesses)', fontsize=16)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(batch_images[:10])\n",
    "    predicted_labels = predictions.argmax(dim=1)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    image = batch_images[i].squeeze().numpy()\n",
    "    true_label = batch_labels[i].item()\n",
    "    pred_label = predicted_labels[i].item()\n",
    "    \n",
    "    ax.imshow(image, cmap='gray')\n",
    "    \n",
    "    # Color: green if correct, red if wrong\n",
    "    color = 'green' if pred_label == true_label else 'red'\n",
    "    ax.set_title(f\"True: {true_label}, Pred: {pred_label}\", color=color)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy on this batch\n",
    "correct = (predicted_labels[:10] == batch_labels[:10]).sum().item()\n",
    "print(f\"\\nAccuracy on these 10 samples: {correct}/10 = {correct*10}%\")\n",
    "print(\"(Random guessing would be ~10% accuracy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7df5bb",
   "metadata": {},
   "source": [
    "## 9. Understanding Activation Functions\n",
    "\n",
    "Let's visualize what ReLU (Rectified Linear Unit) does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e99c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "\n",
    "# Apply ReLU\n",
    "y_relu = F.relu(x)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x.numpy(), y_relu.numpy(), linewidth=2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.title('ReLU Activation Function')\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Show the effect on actual neuron outputs\n",
    "plt.subplot(1, 2, 2)\n",
    "sample_values = torch.tensor([-2, -1, 0, 1, 2, 3])\n",
    "plt.bar(range(len(sample_values)), sample_values.numpy(), alpha=0.5, label='Before ReLU')\n",
    "plt.bar(range(len(sample_values)), F.relu(sample_values).numpy(), alpha=0.7, label='After ReLU')\n",
    "plt.xlabel('Neuron')\n",
    "plt.ylabel('Activation')\n",
    "plt.title('ReLU Effect on Sample Neurons')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nReLU: Keeps positive values, sets negative values to zero\")\n",
    "print(\"This introduces non-linearity, allowing the network to learn complex patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db97b4ed",
   "metadata": {},
   "source": [
    "## 10. Move Model to Device (CPU/GPU)\n",
    "\n",
    "Let's prepare the model to use GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a2c0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "print(f\"\\nModel moved to: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87dd370",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "Congratulations! You've completed Exercise 3. You now know:\n",
    "\n",
    "✅ How to define a neural network class in PyTorch\n",
    "\n",
    "✅ How to create layers (Linear) and activation functions (ReLU)\n",
    "\n",
    "✅ How to implement the forward pass\n",
    "\n",
    "✅ How to examine model architecture and count parameters\n",
    "\n",
    "✅ How to test the model with a forward pass\n",
    "\n",
    "✅ What an untrained model's predictions look like (random!)\n",
    "\n",
    "✅ How activation functions introduce non-linearity\n",
    "\n",
    "✅ How to move the model to GPU/CPU\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Architecture**: Input (784) → Dense(128) → ReLU → Dense(64) → ReLU → Dense(10)\n",
    "- **Parameters**: ~110K trainable parameters\n",
    "- **Untrained model**: Makes random predictions (~10% accuracy)\n",
    "- **ReLU**: Critical for learning complex patterns by introducing non-linearity\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In Exercise 4, we'll train this model and watch it learn to recognize digits!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
