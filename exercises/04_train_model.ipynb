{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19b9956",
   "metadata": {},
   "source": [
    "## 1. Setup: Import Libraries and Prepare Data\n",
    "\n",
    "Building on what we learned in previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d5ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c13aa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa15c809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='../data', train=False, download=True, transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f06314d",
   "metadata": {},
   "source": [
    "## 2. Define the Model (from Exercise 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad85a32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create model and move to device\n",
    "model = SimpleNN().to(device)\n",
    "print(\"Model created and moved to device\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483f22d5",
   "metadata": {},
   "source": [
    "## 3. Define Loss Function and Optimizer\n",
    "\n",
    "**Loss Function**: Measures how wrong the model's predictions are.\n",
    "- We use `CrossEntropyLoss` for multi-class classification\n",
    "- It combines softmax and negative log-likelihood\n",
    "\n",
    "**Optimizer**: Updates the model's parameters to reduce the loss.\n",
    "- We use `Adam` (Adaptive Moment Estimation)\n",
    "- Adam is efficient and works well for most problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f6011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (learning rate = 0.001)\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f\"Loss function: {criterion}\")\n",
    "print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "print(f\"Learning rate: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb2712",
   "metadata": {},
   "source": [
    "## 4. Training Function\n",
    "\n",
    "Let's create a function that trains the model for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b66f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \n",
    "    Returns:\n",
    "        avg_loss: Average loss for the epoch\n",
    "        accuracy: Training accuracy for the epoch\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Move data to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track statistics\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += pred.eq(target).sum().item()\n",
    "        total += target.size(0)\n",
    "        \n",
    "        # Print progress every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'  Batch {batch_idx}/{len(train_loader)}, '\n",
    "                  f'Loss: {loss.item():.4f}, '\n",
    "                  f'Accuracy: {100. * correct / total:.2f}%')\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "print(\"Training function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e96d75",
   "metadata": {},
   "source": [
    "## 5. Test Function\n",
    "\n",
    "Let's create a function to evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set.\n",
    "    \n",
    "    Returns:\n",
    "        avg_loss: Average loss on test set\n",
    "        accuracy: Test accuracy\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # No gradient computation during evaluation\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "print(\"Test function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b96232c",
   "metadata": {},
   "source": [
    "## 6. Train the Model!\n",
    "\n",
    "Now let's train for multiple epochs and track the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690a629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "num_epochs = 10\n",
    "\n",
    "# Track history\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "print(f\"Starting training for {num_epochs} epochs...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, device, train_loader, optimizer, criterion, epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    \n",
    "    # Test\n",
    "    test_loss, test_acc = test(model, device, test_loader, criterion)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\n  Training   - Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
    "    print(f\"  Test       - Loss: {test_loss:.4f}, Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"Training complete! Total time: {training_time:.2f} seconds\")\n",
    "print(f\"Final test accuracy: {test_accuracies[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87dc85e",
   "metadata": {},
   "source": [
    "## 7. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1958e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(range(1, num_epochs + 1), train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "ax1.plot(range(1, num_epochs + 1), test_losses, 'r-', label='Test Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Test Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(range(1, num_epochs + 1), train_accuracies, 'b-', label='Training Accuracy', linewidth=2)\n",
    "ax2.plot(range(1, num_epochs + 1), test_accuracies, 'r-', label='Test Accuracy', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Test Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nImprovement:\")\n",
    "print(f\"  Test accuracy improved from {test_accuracies[0]:.2f}% to {test_accuracies[-1]:.2f}%\")\n",
    "print(f\"  Gain: +{test_accuracies[-1] - test_accuracies[0]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d266ba",
   "metadata": {},
   "source": [
    "## 8. Test Predictions on Sample Images\n",
    "\n",
    "Let's see how the trained model performs on some test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a092925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of test images\n",
    "test_batch, test_labels = next(iter(test_loader))\n",
    "test_batch = test_batch.to(device)\n",
    "test_labels = test_labels.to(device)\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_batch)\n",
    "    predictions = outputs.argmax(dim=1)\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "fig.suptitle('Trained Model Predictions', fontsize=16)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(test_batch):\n",
    "        image = test_batch[i].cpu().squeeze().numpy()\n",
    "        true_label = test_labels[i].cpu().item()\n",
    "        pred_label = predictions[i].cpu().item()\n",
    "        \n",
    "        ax.imshow(image, cmap='gray')\n",
    "        \n",
    "        # Color: green if correct, red if wrong\n",
    "        color = 'green' if pred_label == true_label else 'red'\n",
    "        ax.set_title(f\"True: {true_label}, Pred: {pred_label}\", color=color, fontsize=10)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy on this batch\n",
    "correct = (predictions == test_labels).sum().item()\n",
    "print(f\"\\nAccuracy on this batch: {correct}/{len(test_batch)} = {100*correct/len(test_batch):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0051a6ec",
   "metadata": {},
   "source": [
    "## 9. Save the Trained Model\n",
    "\n",
    "Let's save the model so we can use it later without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ed126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "model_path = '../models/mnist_model.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"File size: {os.path.getsize(model_path) / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e89995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify we can load the model back\n",
    "loaded_model = SimpleNN().to(device)\n",
    "loaded_model.load_state_dict(torch.load(model_path))\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Test that loaded model gives same predictions\n",
    "with torch.no_grad():\n",
    "    test_output = loaded_model(test_batch[:5])\n",
    "    test_preds = test_output.argmax(dim=1)\n",
    "\n",
    "print(f\"Sample predictions from loaded model: {test_preds.cpu().numpy()}\")\n",
    "print(f\"Original predictions:                 {predictions[:5].cpu().numpy()}\")\n",
    "print(\"\\n✓ Loaded model works correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849f6983",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "Congratulations! You've completed Exercise 4. You now know:\n",
    "\n",
    "✅ How to define a loss function (CrossEntropyLoss)\n",
    "\n",
    "✅ How to choose and configure an optimizer (Adam)\n",
    "\n",
    "✅ How to implement a training loop\n",
    "\n",
    "✅ How to track training progress (loss and accuracy)\n",
    "\n",
    "✅ How to evaluate the model on test data\n",
    "\n",
    "✅ How to visualize training progress\n",
    "\n",
    "✅ How to save and load trained models\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Training Loop**: Forward pass → Compute loss → Backward pass → Update weights\n",
    "- **Loss Function**: CrossEntropyLoss for multi-class classification\n",
    "- **Optimizer**: Adam efficiently updates weights to minimize loss\n",
    "- **Epochs**: Multiple passes through the entire dataset\n",
    "- **Accuracy**: Should reach ~97-98% on MNIST with this simple model\n",
    "\n",
    "### Training Progress\n",
    "\n",
    "- Initial accuracy: ~10% (random guessing)\n",
    "- Final accuracy: ~97-98%\n",
    "- The model learned to recognize handwritten digits!\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In Exercise 5, we'll evaluate the model more thoroughly with a confusion matrix and analyze where it makes mistakes!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
