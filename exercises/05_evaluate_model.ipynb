{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b87cc2",
   "metadata": {},
   "source": [
    "## 1. Setup: Import Libraries and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005e577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fd68ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d63412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "test_dataset = datasets.MNIST(root='../data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6745f6d",
   "metadata": {},
   "source": [
    "## 2. Load the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec20d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture (same as before)\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Load trained model\n",
    "model = SimpleNN().to(device)\n",
    "model.load_state_dict(torch.load('../models/mnist_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "print(\"Trained model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47199155",
   "metadata": {},
   "source": [
    "## 3. Generate Predictions on Entire Test Set\n",
    "\n",
    "Let's run the model on all test samples and collect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8098b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all predictions and true labels\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_probabilities = []\n",
    "\n",
    "print(\"Evaluating model on test set...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        predictions = output.argmax(dim=1)\n",
    "        \n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(target.cpu().numpy())\n",
    "        all_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "all_probabilities = np.array(all_probabilities)\n",
    "\n",
    "print(f\"Evaluated {len(all_predictions)} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a092304c",
   "metadata": {},
   "source": [
    "## 4. Calculate Overall Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae2d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall accuracy\n",
    "correct = (all_predictions == all_labels).sum()\n",
    "total = len(all_labels)\n",
    "accuracy = 100.0 * correct / total\n",
    "\n",
    "print(f\"Overall Test Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Correct predictions: {correct}/{total}\")\n",
    "print(f\"Incorrect predictions: {total - correct}/{total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2aa46f",
   "metadata": {},
   "source": [
    "## 5. Per-Class Accuracy\n",
    "\n",
    "Let's see how well the model performs on each digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45ea626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy for each digit\n",
    "print(\"Per-Digit Accuracy:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "digit_accuracies = []\n",
    "\n",
    "for digit in range(10):\n",
    "    # Get indices for this digit\n",
    "    digit_indices = all_labels == digit\n",
    "    digit_preds = all_predictions[digit_indices]\n",
    "    digit_true = all_labels[digit_indices]\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    digit_correct = (digit_preds == digit_true).sum()\n",
    "    digit_total = len(digit_true)\n",
    "    digit_acc = 100.0 * digit_correct / digit_total\n",
    "    digit_accuracies.append(digit_acc)\n",
    "    \n",
    "    print(f\"Digit {digit}: {digit_acc:.2f}% ({digit_correct}/{digit_total})\")\n",
    "\n",
    "# Visualize per-digit accuracy\n",
    "plt.figure(figsize=(12, 5))\n",
    "bars = plt.bar(range(10), digit_accuracies, color='steelblue', edgecolor='black')\n",
    "\n",
    "# Color the bars - green for highest, red for lowest\n",
    "max_idx = np.argmax(digit_accuracies)\n",
    "min_idx = np.argmin(digit_accuracies)\n",
    "bars[max_idx].set_color('green')\n",
    "bars[min_idx].set_color('red')\n",
    "\n",
    "plt.xlabel('Digit')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Per-Digit Accuracy')\n",
    "plt.xticks(range(10))\n",
    "plt.ylim(90, 100)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, acc in enumerate(digit_accuracies):\n",
    "    plt.text(i, acc + 0.2, f'{acc:.1f}%', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest performance: Digit {max_idx} ({digit_accuracies[max_idx]:.2f}%)\")\n",
    "print(f\"Worst performance: Digit {min_idx} ({digit_accuracies[min_idx]:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbfed5f",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix\n",
    "\n",
    "A confusion matrix shows which digits get confused with which."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c647cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=range(10), yticklabels=range(10),\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.title('Confusion Matrix', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nReading the confusion matrix:\")\n",
    "print(\"- Diagonal values (top-left to bottom-right) are correct predictions\")\n",
    "print(\"- Off-diagonal values are misclassifications\")\n",
    "print(\"- Row = True label, Column = Predicted label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5a1166",
   "metadata": {},
   "source": [
    "## 7. Analyze Common Misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4787126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most common confusions (excluding correct predictions)\n",
    "print(\"Most Common Misclassifications:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "confusions = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            confusions.append((i, j, cm[i, j]))\n",
    "\n",
    "# Sort by count\n",
    "confusions.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Print top 10 confusions\n",
    "for true_label, pred_label, count in confusions[:10]:\n",
    "    print(f\"{true_label} misclassified as {pred_label}: {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f694e9a",
   "metadata": {},
   "source": [
    "## 8. Visualize Correct Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9f26d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find correctly classified examples\n",
    "correct_indices = np.where(all_predictions == all_labels)[0]\n",
    "\n",
    "# Sample 12 random correct predictions\n",
    "sample_correct = np.random.choice(correct_indices, 12, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n",
    "fig.suptitle('Correctly Classified Examples', fontsize=16, color='green')\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    img_idx = sample_correct[idx]\n",
    "    image, label = test_dataset[img_idx]\n",
    "    prediction = all_predictions[img_idx]\n",
    "    confidence = all_probabilities[img_idx][prediction] * 100\n",
    "    \n",
    "    ax.imshow(image.squeeze().numpy(), cmap='gray')\n",
    "    ax.set_title(f'True: {label}, Pred: {prediction}\\nConf: {confidence:.1f}%', \n",
    "                 color='green', fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a49fa8",
   "metadata": {},
   "source": [
    "## 9. Visualize Incorrect Predictions\n",
    "\n",
    "This is where we learn the most - by looking at mistakes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca2c5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find incorrectly classified examples\n",
    "incorrect_indices = np.where(all_predictions != all_labels)[0]\n",
    "\n",
    "print(f\"Total incorrect predictions: {len(incorrect_indices)}\")\n",
    "\n",
    "# Sample up to 12 incorrect predictions\n",
    "num_samples = min(12, len(incorrect_indices))\n",
    "sample_incorrect = np.random.choice(incorrect_indices, num_samples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n",
    "fig.suptitle('Incorrectly Classified Examples', fontsize=16, color='red')\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    if idx < len(sample_incorrect):\n",
    "        img_idx = sample_incorrect[idx]\n",
    "        image, label = test_dataset[img_idx]\n",
    "        prediction = all_predictions[img_idx]\n",
    "        confidence = all_probabilities[img_idx][prediction] * 100\n",
    "        \n",
    "        ax.imshow(image.squeeze().numpy(), cmap='gray')\n",
    "        ax.set_title(f'True: {label}, Pred: {prediction}\\nConf: {confidence:.1f}%', \n",
    "                     color='red', fontsize=10)\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Look at these mistakes carefully!\")\n",
    "print(\"Some may be ambiguous even for humans.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f54f8",
   "metadata": {},
   "source": [
    "## 10. Analyze Model Confidence\n",
    "\n",
    "Let's see how confident the model is in its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7996659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get confidence for each prediction (max probability)\n",
    "confidences = np.max(all_probabilities, axis=1)\n",
    "\n",
    "# Separate confidences for correct and incorrect predictions\n",
    "correct_mask = all_predictions == all_labels\n",
    "correct_confidences = confidences[correct_mask]\n",
    "incorrect_confidences = confidences[~correct_mask]\n",
    "\n",
    "print(f\"Average confidence (correct predictions): {correct_confidences.mean():.3f}\")\n",
    "print(f\"Average confidence (incorrect predictions): {incorrect_confidences.mean():.3f}\")\n",
    "\n",
    "# Plot confidence distributions\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(correct_confidences, bins=50, color='green', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Confidence Distribution - Correct Predictions')\n",
    "plt.axvline(correct_confidences.mean(), color='red', linestyle='--', \n",
    "            label=f'Mean: {correct_confidences.mean():.3f}')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(incorrect_confidences, bins=50, color='red', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Confidence Distribution - Incorrect Predictions')\n",
    "plt.axvline(incorrect_confidences.mean(), color='blue', linestyle='--',\n",
    "            label=f'Mean: {incorrect_confidences.mean():.3f}')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInsights:\")\n",
    "print(\"- Correct predictions typically have higher confidence\")\n",
    "print(\"- Some incorrect predictions still have high confidence (overconfident mistakes!)\")\n",
    "print(\"- Low confidence predictions might indicate ambiguous/difficult cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c2ab9",
   "metadata": {},
   "source": [
    "## 11. Classification Report\n",
    "\n",
    "Let's get a detailed per-class performance report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4363358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "report = classification_report(all_labels, all_predictions, \n",
    "                               target_names=[str(i) for i in range(10)])\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\" * 70)\n",
    "print(report)\n",
    "\n",
    "print(\"\\nMetrics explanation:\")\n",
    "print(\"- Precision: Of all predictions for a digit, how many were correct?\")\n",
    "print(\"- Recall: Of all actual instances of a digit, how many did we find?\")\n",
    "print(\"- F1-score: Harmonic mean of precision and recall\")\n",
    "print(\"- Support: Number of true instances for each digit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08bfd59",
   "metadata": {},
   "source": [
    "## 12. Find Most Confident Mistakes\n",
    "\n",
    "Let's look at cases where the model was very confident but wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24de3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find incorrect predictions with highest confidence\n",
    "incorrect_mask = all_predictions != all_labels\n",
    "incorrect_idx = np.where(incorrect_mask)[0]\n",
    "incorrect_conf = confidences[incorrect_mask]\n",
    "\n",
    "# Sort by confidence\n",
    "sorted_indices = incorrect_idx[np.argsort(incorrect_conf)[::-1]]\n",
    "\n",
    "# Show top 8 most confident mistakes\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "fig.suptitle('Most Confident Incorrect Predictions', fontsize=16, color='darkred')\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    if idx < min(8, len(sorted_indices)):\n",
    "        img_idx = sorted_indices[idx]\n",
    "        image, label = test_dataset[img_idx]\n",
    "        prediction = all_predictions[img_idx]\n",
    "        confidence = confidences[img_idx] * 100\n",
    "        \n",
    "        ax.imshow(image.squeeze().numpy(), cmap='gray')\n",
    "        ax.set_title(f'True: {label}, Pred: {prediction}\\nConf: {confidence:.1f}%', \n",
    "                     color='darkred', fontsize=11, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"These are the model's most confident mistakes.\")\n",
    "print(\"Often these are truly ambiguous cases or poorly written digits!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850843ca",
   "metadata": {},
   "source": [
    "## 13. Summary\n",
    "\n",
    "Congratulations! You've completed Exercise 5. You now know:\n",
    "\n",
    "✅ How to calculate overall and per-class accuracy\n",
    "\n",
    "✅ How to create and interpret a confusion matrix\n",
    "\n",
    "✅ How to identify common misclassifications\n",
    "\n",
    "✅ How to analyze model confidence\n",
    "\n",
    "✅ How to visualize correct and incorrect predictions\n",
    "\n",
    "✅ How to find the model's most confident mistakes\n",
    "\n",
    "✅ How to generate a detailed classification report\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "From our evaluation, we learned:\n",
    "\n",
    "1. **Overall Performance**: The model achieves ~97-98% accuracy on MNIST\n",
    "2. **Per-Digit Performance**: Some digits are easier to recognize than others\n",
    "3. **Common Confusions**: Certain digit pairs are commonly confused (e.g., 4/9, 3/8, 7/9)\n",
    "4. **Confidence**: Correct predictions have higher average confidence\n",
    "5. **Edge Cases**: Some mistakes are on genuinely ambiguous examples\n",
    "\n",
    "### Why Evaluation Matters\n",
    "\n",
    "- **Understanding Weaknesses**: Know where your model fails\n",
    "- **Building Trust**: Verify the model works as expected\n",
    "- **Identifying Improvements**: Find areas for enhancement\n",
    "- **Real-world Deployment**: Understand confidence and reliability\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In Exercise 6, we'll use the model for inference - making predictions on new images!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
